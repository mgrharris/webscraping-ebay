{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michael Harris\n",
    "# Web-Scraping President's Day deals from ebay.com\n",
    "# February 13, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the eBay president's Day deals URL, we'll loop through the first 10 pages of results and write all of the results to a text file. Then, we'll download the text files into a BeautifulSoup object, extract relevant information from each listing, and write the results into a table on our local MySQL instance. ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the first 10 pages and return the URLs of non-sponsored items. Save these into an object 'url_list' to later\n",
    "# be called to write all URLs to a text file. \n",
    "# Note that I tried to find any sponsored items on this page while using incognito and couldn't find any, so I extracted all \n",
    "# URLs.\n",
    "\n",
    "url_list = []\n",
    "\n",
    "for i in range(1,10):\n",
    "    url6 = \"https://www.ebay.com/e/daily-deals/hiw-presidents-day-deals-white-sale?_pgn=\"+str(i)\n",
    "    header = ({'User-Agent':'Mozilla/5.0'}) \n",
    "    response = requests.get(url6, headers = header)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    items = soup.findAll('div',{\"class\":\"s-item__info clearfix\"})\n",
    "    count = 0\n",
    "    for item in items:\n",
    "        a = item.find(\"a\")\n",
    "        if not a:\n",
    "            continue\n",
    "        text = item.find(\"span\").get_text\n",
    "        url_simplify = re.sub(\"(.*)\\\\?.*\", r\"\\1\", a['href']) \n",
    "        url_list.append(url_simplify + \"\\n\")\n",
    "        print(url_simplify)\n",
    "                \n",
    "\n",
    "\n",
    "# Print URLs to confirm that we loaded them correctly into text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll open the url_list object that we created, and for each line, write down the URL and then indent to another line.\n",
    "# Save in 'deals.txt' file.\n",
    "with open ('deals.txt', 'w') as file:\n",
    "                for line in url_list:\n",
    "                    file.write(line + \"\\n\")\n",
    "                    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open 'deals.txt' and save each URL into a html file, naming each file file the item ID of the item that we're scraping using \n",
    "# Regular Expressions. To pass through errors, set errors = 'ignore', so that our code doesn't break if a URL isn't valid.\n",
    "\n",
    "with open('deals.txt', encoding = 'UTF-8',errors=\"ignore\") as inf:\n",
    "    for url in inf: \n",
    "        if 'http' in url:\n",
    "            page = requests.get(url.strip())\n",
    "            response = urlopen(url)\n",
    "            page = response.read()\n",
    "            text_after = re.sub('(.+?)/(.+?)/(.+?)/(.+?)/(.+)\\n', '\\g<5>' , url)\n",
    "            with open(text_after + '.htm', 'wb') as outfile:\n",
    "                outfile.write(page)\n",
    "                time.sleep(4)\n",
    "                \n",
    "# Adding time.sleep command to avoid overloading servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've named the files by their item name, have downloaded the HTML from the webpage for each URL, and have saved\n",
    "# each file individually in a folder called 'deals', we can write a loop to loop through the the downloaded pages and \n",
    "# parse them into a BeautifulSoup object.\n",
    "\n",
    "# Create list objects to store results of the for loop iterations, that we'll later use to import to MySQL. \n",
    "seller_list = []\n",
    "sellerrating_list = []\n",
    "item_price_list = []\n",
    "list_price_list = []\n",
    "numbersold_list = [] \n",
    "condition_list = []\n",
    "\n",
    "\n",
    "\n",
    "# Loop through html files in the 'deals' directory.\n",
    "directory ='/Users/MIckey Harris/Downloads/deals'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.htm'):\n",
    "        fname = os.path.join(directory,filename)\n",
    "        with open(fname, encoding = 'UTF-8', errors = 'ignore') as f:\n",
    "            soup = BeautifulSoup(f.read(),'html.parser')\n",
    "            seller = soup.findAll('div', {'class': 'mbg vi-VR-margBtm3'}) # Found all class objects that contained seller name.\n",
    "            for item in seller:\n",
    "                b = item.find('span').get_text()                          # Returned text for all sellers.\n",
    "                seller_list.append(b)\n",
    "            sellerrating = soup.findAll('span', {'class': 'mbg-l'})       # Found all class objects that contained seller rating.\n",
    "            for item in sellerrating:\n",
    "                a = item.find('a').get_text()                             # Returned text for all seller ratings.\n",
    "                sellerrating_list.append(a)    \n",
    "            item_price = soup.findAll('div', {'id': 'vi-mskumap-none'})   # Found all div objects w/ ID that contained item price.\n",
    "            for item in item_price:\n",
    "                c = item.find('span')['content']                          # Returned text for all item prices.\n",
    "                item_price_list.append(float(c))\n",
    "            list_price = soup.findAll('div', {'id':'vi-priceDetails'})    # Found all div objects w/ ID that contained list price.\n",
    "            for item in list_price:\n",
    "                h = item.find('span',{'class': 'vi-originalPrice'}).get_text() # Returned text for all list prices.\n",
    "                list_price_list.append(h)\n",
    "            numbersold = soup.findAll('a', {'class': 'vi-txt-underline'}) # Found all a objects w/ a certain class that contained number sold.\n",
    "            for item in numbersold: \n",
    "               d = item.get_text()\n",
    "               d_simplify = re.sub(\"(.+?)(.+)\\n\", '\\g<1>', d)             # Returned text for all list prices. Use regex to format as number.\n",
    "               numbersold_list.append(d_simplify)\n",
    "            condition = soup.findAll('div',{'class':'u-flL condText'})    # Found all div objects w/ a certain class that contained condition.\n",
    "            for item in condition:\n",
    "                g = item.get_text()                                       # Returned text for all item conditions.\n",
    "                condition_list.append(g)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "print(seller_list)\n",
    "print(sellerrating_list)\n",
    "print(item_price_list)\n",
    "print(list_price_list)\n",
    "print(condition_list)\n",
    "print(numbersold_list)\n",
    "\n",
    "# We observe that our variables have been stored into list objects as we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the lists that we imported are the same length by replacing missing values with 'Null'. \n",
    "\n",
    "# First, I'm ensuring that my lists are the same length so that I can import to MySQL. \n",
    "# Next, I am replacing missing values with 'NULL'. \n",
    "\n",
    "max_length = max(max(len(seller_list), len(sellerrating_list)), len(item_price_list), len(list_price_list), len(condition_list), len(numbersold_list))\n",
    "seller_list += ['NULL'] * (max_length - len(seller_list))\n",
    "sellerrating_list += ['NULL'] * (max_length - len(sellerrating_list))\n",
    "item_price_list += ['NULL'] * (max_length - len(item_price_list))\n",
    "list_price_list += ['NULL'] * (max_length - len(list_price_list))\n",
    "condition_list += ['NULL'] * (max_length - len(condition_list))\n",
    "numbersold_list += ['NULL'] * (max_length - len(numbersold_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to our MySQL local instance & set up our cursor. Connect to 'msba' database.\n",
    "\n",
    "db = mysql.connect(\n",
    "    host = \"localhost\",\n",
    "    user = \"root\",\n",
    "    passwd = \"\",\n",
    "    database = 'msba') \n",
    "\n",
    "cursor = db.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 'deals' table with SellerName, SellerRating, ItemPrice, ListPrice, NumberSold, and Condition columns. \n",
    "\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS deals (sellername VARCHAR(50), sellerrating VARCHAR(50), itemprice FLOAT, listprice VARCHAR(50), numbersold VARCHAR(50), cond VARCHAR(50))\")\n",
    "\n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "\n",
    "# Fetch all tables within our database.\n",
    "tables = cursor.fetchall() \n",
    "\n",
    "# Showing all our tables in the database.\n",
    "for table in tables:\n",
    "    print(table)\n",
    "    \n",
    "# We see that the 'deals' table has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PART 3E ###\n",
    "\n",
    "# List insert query to insert into the 'deals' table our eBay data.\n",
    "query = \"INSERT INTO deals (sellername,sellerrating,itemprice,listprice,numbersold,cond ) VALUES (%s, %s, %s, %s, %s, %s)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a for loop to execute insert queries for each of the 5 records that are stored in the 'data' object.\n",
    "\n",
    "# In this case, I'll simply test that importing my list works by importing the first 5 items in the list.\n",
    "data = [\n",
    "    (seller_list[0], sellerrating_list[0],item_price_list[0],list_price_list[0],numbersold_list[0],condition_list[0]),\n",
    "    (seller_list[1], sellerrating_list[1],item_price_list[1],list_price_list[1],numbersold_list[1],condition_list[1]),\n",
    "    (seller_list[2], sellerrating_list[2],item_price_list[2],list_price_list[2],numbersold_list[2],condition_list[2]),\n",
    "    (seller_list[3], sellerrating_list[3],item_price_list[3],list_price_list[3],numbersold_list[3],condition_list[3]),\n",
    "    (seller_list[4], sellerrating_list[4],item_price_list[4],list_price_list[4],numbersold_list[4],condition_list[4])\n",
    "]\n",
    "\n",
    "for values in data:\n",
    "    cursor.execute(query,values)\n",
    "\n",
    "\n",
    "# Next, we run the 'commit()' method of the database object to ensure that our insert statement executes. \n",
    "db.commit()\n",
    "\n",
    "# Lastly, print that our records were inserted correctly into the database.\n",
    "print(cursor.rowcount, \"records inserted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print select query from our database showing that our data was correctly inserted.\n",
    "query = \"SELECT * FROM deals\"\n",
    "\n",
    "# Execute the query.\n",
    "cursor.execute(query)\n",
    "\n",
    "# Return all the records in the cursor object.\n",
    "records = cursor.fetchall()\n",
    "\n",
    "# Show the data as an output.\n",
    "for record in records:\n",
    "    print(record)\n",
    "\n",
    "# We see that our records have been imported into my MySQL table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can write queries from our 'deals' table to determine better purchase decisions. Let's query the average price,\n",
    "# minumum price, and maximum price.\n",
    "\n",
    "query = \"SELECT AVG(itemprice), min(itemprice), max(itemprice) FROM deals\"\n",
    "\n",
    "# Execute the query.\n",
    "cursor.execute(query)\n",
    "\n",
    "# Return all the records in the cursor object.\n",
    "records = cursor.fetchall()\n",
    "\n",
    "# Show the data as an output.\n",
    "for record in records:\n",
    "    print(record)\n",
    "    \n",
    "# Returned are the summary statistics for the ItemPrice column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example, I'll group by sellername, and return summary statistics for the sellers that I imported.\n",
    "query = \"SELECT AVG(itemprice), min(itemprice), max(itemprice) FROM deals GROUP BY sellername\"\n",
    "\n",
    "# Execute the query.\n",
    "cursor.execute(query)\n",
    "\n",
    "# Return all the records in the cursor object.\n",
    "records = cursor.fetchall()\n",
    "\n",
    "# Show the data as an output.\n",
    "for record in records:\n",
    "    print(record)\n",
    "    \n",
    "# Returned are the summary statistics for the ItemPrice column. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
